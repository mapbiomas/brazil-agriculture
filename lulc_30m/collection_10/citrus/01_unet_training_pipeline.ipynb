{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# USE THE GPUS-T4 ENVIRONMENT (HIGH RAM)"
      ],
      "metadata": {
        "id": "O-4CJKOJrEWI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RAW DATASET EXPORT"
      ],
      "metadata": {
        "id": "Q9IBlxfIkBYB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install geedim"
      ],
      "metadata": {
        "id": "qvGKEe7p3VJB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ee\n",
        "from geedim.mask import MaskedImage\n",
        "import geedim as gd\n",
        "import geemap\n",
        "import os\n",
        "\n",
        "ee.Authenticate()\n",
        "ee.Initialize(project='mapbiomas-remap-sentinel')\n",
        "\n",
        "\n",
        "gd.Initialize()"
      ],
      "metadata": {
        "id": "oPrvCVKk3awi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Complete grid of the area containing the references\n",
        "\n",
        "gridCitrus = [ee.Feature(\n",
        "            ee.Geometry.Polygon(\n",
        "                [[[-48.70588801192858, -20.035935384858842],\n",
        "                  [-49.57930109786608, -20.350421721213056],\n",
        "                  [-49.54634211349108, -20.931300558885997],\n",
        "                  [-48.77729914474108, -21.090266628321324],\n",
        "                  [-48.75532648849108, -21.56614118525384],\n",
        "                  [-50.29890559005358, -22.223641598287102],\n",
        "                  [-50.24946711349108, -22.629142663970672],\n",
        "                  [-49.04646418380358, -23.14533155137643],\n",
        "                  [-49.26619074630358, -23.55383721044761],\n",
        "                  [-49.05745051192858, -23.68972531577456],\n",
        "                  [-48.78279230880358, -23.37243440542318],\n",
        "                  [-48.54658625411608, -23.558872615027653],\n",
        "                  [-46.66792414474108, -21.89200397442101],\n",
        "                  [-47.14033625411608, -21.243221782473224],\n",
        "                  [-48.01374934005358, -21.437649544995725]]]),\n",
        "            {\n",
        "              \"system:index\": \"0\"\n",
        "            })]"
      ],
      "metadata": {
        "id": "Ymj6o7uC6SdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "32mhZEvkuxCq"
      },
      "outputs": [],
      "source": [
        "# Configurações\n",
        "startDate = '2020-01-01'\n",
        "endDate = '2020-12-31'\n",
        "\n",
        "\n",
        "chirp_scale, chirp_size = 30, 1024\n",
        "chirp_size_m = chirp_scale * chirp_size\n",
        "\n",
        "cloudCoverValue = 80\n",
        "uf_code = 'SP'\n",
        "\n",
        "output_folder = f'DATASET_CITRUS_PERC_{uf_code}'\n",
        "\n",
        "# collection and input layers\n",
        "ref_map = ee.FeatureCollection('projects/assets/reference_map')\n",
        "estados = ee.FeatureCollection('regions/ibge_estados_2019')\n",
        "proj = gridCitrus.first().geometry().projection()\n",
        "\n",
        "# Funções\n",
        "def filter_landsat(path, roi, start, end, cloud_max):\n",
        "    return ee.ImageCollection(path) \\\n",
        "        .filterDate(start, end) \\\n",
        "        .filterBounds(roi) \\\n",
        "        .filter(ee.Filter.lt('CLOUD_COVER_LAND', cloud_max))\n",
        "\n",
        "\n",
        "def padronize_band_names(image):\n",
        "    spacecraft_id = image.get('SPACECRAFT_ID')\n",
        "\n",
        "    old_band_names = ee.Dictionary({\n",
        "        'LANDSAT_5': ['B1', 'B2', 'B3', 'B4', 'B5', 'B7', 'B6', 'QA_PIXEL'],\n",
        "        'LANDSAT_7': ['B1', 'B2', 'B3', 'B4', 'B5', 'B7', 'B6_VCID_1', 'QA_PIXEL'],\n",
        "        'LANDSAT_8': ['B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B10', 'QA_PIXEL'],\n",
        "        'LANDSAT_9': ['B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B10', 'QA_PIXEL']\n",
        "    }).get(spacecraft_id)\n",
        "\n",
        "    new_band_names = ['blue', 'green', 'red', 'nir', 'swir1', 'swir2', 'tir1', 'BQA']\n",
        "\n",
        "    return ee.Algorithms.If(\n",
        "        old_band_names,\n",
        "        image.select(ee.List(old_band_names), new_band_names),\n",
        "        image\n",
        "    )\n",
        "\n",
        "\n",
        "def mask_clouds(image):\n",
        "    qa = image.select('BQA')\n",
        "    mask = (qa.bitwiseAnd(1 << 3)\n",
        "            .And(qa.bitwiseAnd(1 << 8).Or(qa.bitwiseAnd(1 << 9)))\n",
        "            .Or(qa.bitwiseAnd(1 << 1))\n",
        "            .Or(qa.bitwiseAnd(1 << 4).And(qa.bitwiseAnd(1 << 10).Or(qa.bitwiseAnd(1 << 11))))\n",
        "            .Or(qa.bitwiseAnd(1 << 5))\n",
        "            .Or(qa.bitwiseAnd(1 << 7))\n",
        "            .Or(qa.bitwiseAnd(1 << 14).And(qa.bitwiseAnd(1 << 15))))\n",
        "    return image.updateMask(mask.Not())\n",
        "\n",
        "def normalize_band(band_name, image, p1, p99):\n",
        "    return image.select(band_name).unitScale(p1, p99).clamp(0, 1).rename(f'{band_name}_norm')\n",
        "\n",
        "def get_evi2(image):\n",
        "    evi2 = image.expression(\n",
        "        '2.5 * (NIR - RED) / (NIR + 2.4 * RED + 1)',\n",
        "        {\n",
        "            'NIR': image.select('nir_norm'),\n",
        "            'RED': image.select('red_norm')\n",
        "        }).rename('evi2')\n",
        "    return image.addBands(evi2)\n",
        "\n",
        "def get_ndwi(image):\n",
        "    ndwi = image.expression(\n",
        "        '(NIR - SWIR1) / (NIR + SWIR1)',\n",
        "        {\n",
        "            'NIR': image.select('nir_norm'),\n",
        "            'SWIR1': image.select('swir1_norm')\n",
        "        }).rename('ndwi')\n",
        "    return image.addBands(ndwi)\n",
        "\n",
        "# Interest region\n",
        "roi = estados.filter(ee.Filter.eq('SIGLA_UF', uf_code))\n",
        "\n",
        "\n",
        "# Polygon division function (split_pol)\n",
        "def split_pol(ft):\n",
        "    id_property_name_in_grid_rice = 'id'\n",
        "    ft_original_id_val = ft.get(id_property_name_in_grid_rice)\n",
        "\n",
        "    id_value_computed = ee.Algorithms.If(\n",
        "        ft_original_id_val,\n",
        "        ft_original_id_val,\n",
        "        ee.String('grid_').cat(ee.String(ft.get('system:index')))\n",
        "    )\n",
        "    ft_original_id_eeString = ee.String(id_value_computed)\n",
        "\n",
        "    geom_reproject = ft.transform(proj.atScale(chirp_size), 1)\n",
        "\n",
        "    def map_over_cells(ftg):\n",
        "        ftg = ee.Feature(ftg)\n",
        "        cell_idx = ee.String(ftg.get('system:index')).split(',').join('_').replace('-', '1')\n",
        "        unique_id_for_export = ft_original_id_eeString.cat('_').cat(cell_idx)\n",
        "        return ftg.copyProperties(ft).set('id', unique_id_for_export)\n",
        "\n",
        "    return geom_reproject.geometry().coveringGrid(proj, chirp_size_m).map(map_over_cells)\n",
        "\n",
        "\n",
        "bigs_splitted = gridCitrus.map(split_pol).flatten()\n",
        "\n",
        "\n",
        "print(gridCitrus.getInfo())\n",
        "\n",
        "# Reference\n",
        "reference = ee.Image(0).paint(ref_map, 1).rename('reference').clip(roi)\n",
        "\n",
        "\n",
        "# Collection Landsat\n",
        "l5 = filter_landsat(\"LANDSAT/LT05/C02/T1_TOA\", roi, \"2000-01-01\", \"2011-10-01\", cloudCoverValue)\n",
        "l7a = filter_landsat(\"LANDSAT/LE07/C02/T1_TOA\", roi, \"2000-01-01\", \"2003-05-31\", cloudCoverValue)\n",
        "l7b = filter_landsat(\"LANDSAT/LE07/C02/T1_TOA\", roi, \"2011-10-01\", \"2013-03-01\", cloudCoverValue)\n",
        "l8 = filter_landsat(\"LANDSAT/LC08/C02/T1_TOA\", roi, \"2013-03-01\", \"2030-01-01\", cloudCoverValue)\n",
        "l9 = filter_landsat(\"LANDSAT/LC09/C02/T1_TOA\", roi, \"2019-03-01\", \"2030-01-01\", cloudCoverValue)\n",
        "\n",
        "# Assemble and process the collection\n",
        "collection = l8.merge(l9).merge(l7a).merge(l7b).merge(l5) \\\n",
        "    .map(lambda img: ee.Image(padronize_band_names(img))) \\\n",
        "    .map(mask_clouds) \\\n",
        "    .filterDate(startDate, endDate)\n",
        "\n",
        "median = collection.median()\n",
        "\n",
        "# Calculates percentiles in the reference area\n",
        "masked = median.updateMask(reference)\n",
        "bands = ['blue', 'green', 'red', 'nir', 'swir1', 'swir2']\n",
        "percentiles = masked.select(bands).reduceRegion(\n",
        "    reducer=ee.Reducer.percentile([1, 99]),\n",
        "    geometry=roi.geometry(),\n",
        "    scale=chirp_scale,\n",
        "    maxPixels=1e13\n",
        ")\n",
        "\n",
        "\n",
        "# Normalized bands\n",
        "norm_bands = []\n",
        "for b in bands:\n",
        "    p1 = ee.Number(percentiles.get(f'{b}_p1'))\n",
        "    p99 = ee.Number(percentiles.get(f'{b}_p99'))\n",
        "    norm_bands.append(normalize_band(b, median, p1, p99))\n",
        "\n",
        "normalized = ee.Image(norm_bands).toFloat()\n",
        "\n",
        "mosaic_unet = normalized.select(['red_norm', 'nir_norm', 'swir1_norm'])\n",
        "\n",
        "\n",
        "image_to_export = mosaic_unet.unmask().multiply(255).uint8()\n",
        "label_to_export = reference.unmask().byte()\n",
        "\n",
        "\n",
        "final_image_to_export = image_to_export.addBands(label_to_export)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exportation in iiles with geedim\n",
        "\n",
        "grid_list = bigs_splitted.aggregate_array('id').getInfo()\n",
        "\n",
        "output_folder = '/path/tile_export'\n",
        "\n",
        "if not os.path.exists(output_folder):\n",
        "    os.makedirs(output_folder)\n",
        "\n",
        "# 5. Instancia a imagem no geedim\n",
        "# Changed gd.download.BaseImage to gd.MaskedImage\n",
        "gd_image = gd.MaskedImage(final_image_to_export)\n",
        "\n",
        "# 6. Executa o download\n",
        "#    - region: Sua 'grid'. O geedim vai criar um arquivo para cada feature da coleção.\n",
        "#    - filename_pattern: Define como os arquivos serão nomeados. Usamos a propriedade 'id'\n",
        "#      que você criou na função split_pol.\n",
        "#    - scale_factor: Equivalente ao seu .multiply(255).\n",
        "#    - dtype: Equivalente ao seu .uint8().\n",
        "for i, grid_id in enumerate(grid_list):\n",
        "  gd_image.download(\n",
        "      filename=f'{output_folder}/mosaic_{grid_id}_mosaic.tif',\n",
        "      region=bigs_splitted.filter(ee.Filter.eq('id', grid_id)).geometry(),\n",
        "      scale=chirp_scale,\n",
        "      crs='EPSG:3857',\n",
        "      overwrite=True,\n",
        "      bands=['red_norm', 'nir_norm', 'swir1_norm'],\n",
        "      resampling='near',\n",
        "      dtype='uint8',\n",
        "      scale_offset=None\n",
        "  )\n",
        "\n",
        "  gd_image.download(\n",
        "      filename=f'{output_folder}/label_{grid_id}_label.tif',\n",
        "      region=bigs_splitted.filter(ee.Filter.eq('id', grid_id)).geometry(),\n",
        "      scale=chirp_scale,\n",
        "      crs='EPSG:3857',\n",
        "      overwrite=True,\n",
        "      bands=['reference'],\n",
        "      resampling='near',\n",
        "      dtype='uint8',\n",
        "      scale_offset=None\n",
        "  )\n",
        "\n",
        "print(f\"\\nExport started. The imagesAs imagens will be save at path '{output_folder}'.\")\n",
        "print(f\"A file will be generated .tif for each of the {bigs_splitted.size().getInfo()} ggeometries of your grid.\")"
      ],
      "metadata": {
        "id": "SZezH1pUYA1k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# WINDOWED DATASET\n"
      ],
      "metadata": {
        "id": "w10IxmD-kNfx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "This script processes the large GeoTIFF files exported from GEE. It uses a sliding\n",
        "window approach to create smaller, fixed-size patches (e.g., 256x256 pixels).\n",
        "These patches are saved as individual .npy files and will serve as the input for\n",
        "the U-Net model.\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import rasterio\n",
        "import re\n",
        "\n",
        "\n",
        "input_dir = '/path/tile_export'\n",
        "output_img_dir = '/path/tile_export/image'\n",
        "output_label_dir = '/path/tile_export/gt'\n",
        "\n",
        "\n",
        "os.makedirs(output_img_dir, exist_ok=True)\n",
        "os.makedirs(output_label_dir, exist_ok=True)\n",
        "\n",
        "# windowing parameters\n",
        "patch_size = 256\n",
        "#stride = int(patch_size * 0.875)\n",
        "stride = int(patch_size * 0.5)\n",
        "#stride = int(patch_size * 0.125)\n",
        "print(stride)\n",
        "\n",
        "# windowing function\n",
        "def sliding_window(image, label, patch_size, stride):\n",
        "    h, w = image.shape[:2]\n",
        "    for i in range(0, h - patch_size + 1, stride):\n",
        "        for j in range(0, w - patch_size + 1, stride):\n",
        "            img_patch = image[i:i + patch_size, j:j + patch_size]\n",
        "            label_patch = label[i:i + patch_size, j:j + patch_size]\n",
        "\n",
        "            if np.any(label_patch):\n",
        "                yield img_patch, label_patch\n",
        "\n",
        "for file in tqdm(os.listdir(input_dir)):\n",
        "    if file.endswith('mosaic.tif') and file.startswith('mosaic_'):\n",
        "        match = re.match(r'mosaic_(.*)_mosaic\\.tif', file)\n",
        "        if not match:\n",
        "            print(f\"Nome inválido: {file}, pulando.\")\n",
        "            continue\n",
        "        nome_base = match.group(1)\n",
        "\n",
        "        label_file = f\"label_{nome_base}_label.tif\"\n",
        "\n",
        "        img_path = os.path.join(input_dir, file)\n",
        "        label_path = os.path.join(input_dir, label_file)\n",
        "\n",
        "        print(img_path)\n",
        "        print(label_path)\n",
        "\n",
        "        if not os.path.exists(label_path):\n",
        "            print(f\"Rótulo não encontrado para {file}, pulando.\")\n",
        "            continue\n",
        "\n",
        "        with rasterio.open(img_path) as src:\n",
        "            image = src.read()\n",
        "        image = image.transpose(1, 2, 0)\n",
        "\n",
        "        with rasterio.open(label_path) as src:\n",
        "            label = src.read(1)\n",
        "\n",
        "        print(f\"Shape original da imagem: {image.shape}\")\n",
        "        print(f\"Shape original do label: {label.shape}\")\n",
        "\n",
        "\n",
        "        # Apply windowing\n",
        "        for idx, (img_patch, label_patch) in enumerate(sliding_window(image, label, patch_size, stride)):\n",
        "            img_save_path = os.path.join(output_img_dir, f\"{nome_base}_{idx:04d}.npy\")\n",
        "            label_save_path = os.path.join(output_label_dir, f\"{nome_base}_{idx:04d}.npy\")\n",
        "\n",
        "            np.save(img_save_path, img_patch)\n",
        "            np.save(label_save_path, label_patch)\n"
      ],
      "metadata": {
        "id": "NotgtEdlkM5k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "image_files = [f for f in os.listdir(output_img_dir) if f.endswith('.npy')]\n",
        "label_files = [f for f in os.listdir(output_label_dir) if f.endswith('.npy')]\n",
        "\n",
        "\n",
        "image_files.sort()\n",
        "label_files.sort()\n",
        "\n",
        "if len(image_files) != len(label_files):\n",
        "    print(\"Warning: The number of image and label files does not match.\")\n",
        "\n",
        "# Função para visualizar um par de imagem e ground truth\n",
        "def visualize_random_pair():\n",
        "    if not image_files:\n",
        "        print(\"No .npy files found in the specified directories.\")\n",
        "        return\n",
        "\n",
        "    # Escolhe um arquivo aleatório\n",
        "    random_index = random.randint(0, len(image_files) - 1)\n",
        "    img_filename = image_files[random_index]\n",
        "    label_filename = label_files[random_index] # Assume que a ordem corresponde\n",
        "\n",
        "    img_path = os.path.join(output_img_dir, img_filename)\n",
        "    label_path = os.path.join(output_label_dir, label_filename)\n",
        "    # Carrega os dados\n",
        "    try:\n",
        "        image_data = np.load(img_path)\n",
        "        label_data = np.load(label_path)\n",
        "        print(image_data.shape)\n",
        "        print(label_data.shape)\n",
        "        print(np.unique(label_data))\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading {img_path} or {label_path} files: {e}\")\n",
        "        return\n",
        "\n",
        "    # Visualiza\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
        "\n",
        "    # Exibe a imagem. Dependendo das bandas, pode ser necessário ajustar a exibição.\n",
        "    # Se for RGB ou similar (3 canais), visualize diretamente.\n",
        "    # Se for de 6 canais como no exemplo, pode ser necessário selecionar alguns.\n",
        "    # Aqui assumimos que as bandas normais (evi2_norm, swir1_norm, swir2_norm) estão presentes\n",
        "    # e visualizamos as 3 primeiras (evi2_norm, swir1_norm, swir2_norm como RGB)\n",
        "    # Ajuste conforme as bandas que você salvou e deseja visualizar.\n",
        "    if image_data.shape[-1] >= 3:\n",
        "        # Visualiza as 3 primeiras bandas como RGB\n",
        "        axes[0].imshow(image_data[:, :, :3].astype(np.uint8)) # Assumindo valores 0-255\n",
        "    else:\n",
        "        # Se tiver menos de 3 bandas, visualize como grayscale (primeira banda)\n",
        "        axes[0].imshow(image_data[:, :, 0], cmap='gray')\n",
        "\n",
        "    axes[0].set_title(f'Image\\n({img_filename})')\n",
        "    axes[0].axis('off')\n",
        "\n",
        "    # Exibe o ground truth\n",
        "    # Use um cmap apropriado para labels binários (0 ou 1)\n",
        "    axes[1].imshow(label_data, cmap='gray', vmin=0, vmax=1)\n",
        "    axes[1].set_title(f'Ground Truth\\n({label_filename})')\n",
        "    axes[1].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Exemplo de uso: visualize 5 pares aleatórios\n",
        "for _ in range(5):\n",
        "    visualize_random_pair()\n"
      ],
      "metadata": {
        "id": "vUpyXi_qmJdr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MODEL TRAINING"
      ],
      "metadata": {
        "id": "ge2Z5rnVooU6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## UNET"
      ],
      "metadata": {
        "id": "fnjcjCrlm33Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary libraries quietly\n",
        "!pip install -q git+https://github.com/qubvel/segmentation_models.pytorch\n",
        "!pip install -q rasterio\n",
        "!pip install -q torchmetrics"
      ],
      "metadata": {
        "id": "_t90jY5znTXe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    \"\"\"\n",
        "    A standard U-Net architecture for semantic segmentation.\n",
        "    Args:\n",
        "        in_channels (int): Number of input channels (e.g., 3 for RGB).\n",
        "        out_channels (int): Number of output classes (e.g., 1 for binary segmentation).\n",
        "        init_features (int): Number of features in the first convolutional layer.\n",
        "        no_drop (bool): If True, dropout layers are disabled (replaced with Identity).\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels=3, out_channels=1, init_features=64, no_drop=True):\n",
        "        super(UNet, self).__init__()\n",
        "        self.no_drop = no_drop  # Controle global do dropout\n",
        "\n",
        "        features = init_features\n",
        "\n",
        "        # Blocos do Encoder\n",
        "        self.encoder1 = UNet._block(in_channels, features, name=\"enc1\")\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.drop1 = nn.Dropout(0.25) if not no_drop else nn.Identity()\n",
        "\n",
        "        self.encoder2 = UNet._block(features, features * 2, name=\"enc2\")\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.drop2 = nn.Dropout(0.25) if not no_drop else nn.Identity()\n",
        "\n",
        "        self.encoder3 = UNet._block(features * 2, features * 4, name=\"enc3\")\n",
        "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.drop3 = nn.Dropout(0.5) if not no_drop else nn.Identity()\n",
        "\n",
        "        self.encoder4 = UNet._block(features * 4, features * 8, name=\"enc4\")\n",
        "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.drop4 = nn.Dropout(0.5) if not no_drop else nn.Identity()\n",
        "\n",
        "        self.bottleneck = UNet._block(features * 8, features * 16, name=\"bottleneck\")\n",
        "        self.drop5 = nn.Dropout(0.5) if not no_drop else nn.Identity()\n",
        "\n",
        "        self.upconv4 = nn.ConvTranspose2d(features * 16, features * 8, kernel_size=2, stride=2)\n",
        "        self.decoder4 = UNet._block(features * 16, features * 8, name=\"dec4\")\n",
        "\n",
        "        self.upconv3 = nn.ConvTranspose2d(features * 8, features * 4, kernel_size=2, stride=2)\n",
        "        self.decoder3 = UNet._block(features * 8, features * 4, name=\"dec3\")\n",
        "\n",
        "        self.upconv2 = nn.ConvTranspose2d(features * 4, features * 2, kernel_size=2, stride=2)\n",
        "        self.decoder2 = UNet._block(features * 4, features * 2, name=\"dec2\")\n",
        "\n",
        "        self.upconv1 = nn.ConvTranspose2d(features * 2, features, kernel_size=2, stride=2)\n",
        "        self.decoder1 = UNet._block(features * 2, features, name=\"dec1\")\n",
        "\n",
        "        # Camada final\n",
        "        self.conv = nn.Conv2d(in_channels=features, out_channels=out_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Encoder\n",
        "        enc1 = self.drop1(self.encoder1(x))\n",
        "        enc2 = self.drop2(self.encoder2(self.pool1(enc1)))\n",
        "        enc3 = self.drop3(self.encoder3(self.pool2(enc2)))\n",
        "        enc4 = self.drop4(self.encoder4(self.pool3(enc3)))\n",
        "\n",
        "        # Bottleneck\n",
        "        bottleneck = self.drop5(self.bottleneck(self.pool4(enc4)))\n",
        "\n",
        "        # Decoder\n",
        "        dec4 = self.upconv4(bottleneck)\n",
        "        dec4 = torch.cat((dec4, enc4), dim=1)\n",
        "        dec4 = self.decoder4(dec4)  # Sem dropout no decoder\n",
        "\n",
        "        dec3 = self.upconv3(dec4)\n",
        "        dec3 = torch.cat((dec3, enc3), dim=1)\n",
        "        dec3 = self.decoder3(dec3)\n",
        "\n",
        "        dec2 = self.upconv2(dec3)\n",
        "        dec2 = torch.cat((dec2, enc2), dim=1)\n",
        "        dec2 = self.decoder2(dec2)\n",
        "\n",
        "        dec1 = self.upconv1(dec2)\n",
        "        dec1 = torch.cat((dec1, enc1), dim=1)\n",
        "        dec1 = self.decoder1(dec1)\n",
        "\n",
        "        return self.conv(dec1)\n",
        "\n",
        "    @staticmethod\n",
        "    def _block(in_channels, features, name):\n",
        "        \"\"\"\n",
        "        Creates a standard U-Net block: Conv -> BN -> ReLU -> Conv -> BN -> ReLU\n",
        "        \"\"\"\n",
        "        return nn.Sequential(\n",
        "            nn.Conv2d(in_channels, features, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(features),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(features, features, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(features),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )"
      ],
      "metadata": {
        "id": "17_kxzV7m6IV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## UTIL FUNCTIONS"
      ],
      "metadata": {
        "id": "r3Kgw0ZdngrU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import torchvision.transforms as T\n",
        "from torchvision import transforms\n",
        "import random\n",
        "\n",
        "def set_seed(seed=42):\n",
        "    \"\"\"\n",
        "    Sets the seed for reproducibility of the train/validation split.\n",
        "    \"\"\"\n",
        "    random.seed(seed)  # Semente para operações aleatórias do Python\n",
        "    np.random.seed(seed)  # Semente para NumPy\n",
        "    torch.manual_seed(seed)  # Semente para PyTorch\n",
        "\n",
        "# Define a semente ANTES do random_split\n",
        "set_seed(42)\n",
        "\n",
        "class UniqueDataset(Dataset):\n",
        "    def __init__(self, img_dir, gt_dir, transform=None, scale='mm'):\n",
        "        \"\"\"\n",
        "        Dataset for image patches and their corresponding masks.\n",
        "        :param img_dir: Directory with input image patches (.npy files).\n",
        "        :param gt_dir: Directory with ground truth masks (.npy files).\n",
        "        :param transform: PyTorch transforms to be applied.\n",
        "        :param scale: Normalization method ('mm', 'ss', 'mmpc', or 'div255').\n",
        "        \"\"\"\n",
        "        self.img_dir = img_dir\n",
        "        self.gt_dir = gt_dir\n",
        "        self.transform = transform\n",
        "        self.scale = scale\n",
        "        self.img_names = os.listdir(img_dir)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_names)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.img_dir, self.img_names[idx])\n",
        "        gt_path = os.path.join(self.gt_dir, self.img_names[idx])\n",
        "\n",
        "\n",
        "        # Check if files exist before loading\n",
        "        if not os.path.exists(img_path):\n",
        "            print(f\"ERROR: Image file not found: {img_path}\")\n",
        "\n",
        "        if not os.path.exists(gt_path):\n",
        "            print(f\"ERROR: Mask file not found: {gt_path}\")\n",
        "\n",
        "        try:\n",
        "            image = np.load(img_path).astype(np.float32)\n",
        "        except ValueError as e:\n",
        "            print(f\"[ERROR] Failed to load: {img_path}\")\n",
        "            raise e\n",
        "\n",
        "        mask = np.load(gt_path).astype(np.uint8)\n",
        "\n",
        "        # Normalização\n",
        "        if self.scale == 'mm': # Min-Max scaling\n",
        "            for i in range(image.shape[0]):\n",
        "                min_val = image.min()\n",
        "                max_val = image.max()\n",
        "                image[i] = ((image[i] - min_val)) / (max_val - min_val) if max_val > min_val else 0\n",
        "        elif self.scale == 'ss': # Standard score scaling\n",
        "            for i in range(image.shape[0]):\n",
        "                mean_val = image[i].mean()\n",
        "                std_val = image[i].std()\n",
        "                image[i] = ((image[i] - mean_val)) / (std_val)\n",
        "        elif self.scale == 'mmpc': # Min-Max scaling with percentile clipping\n",
        "            for i in range(image.shape[0]):\n",
        "                lower = np.percentile(image[i], 2)\n",
        "                upper = np.percentile(image[i], 98)\n",
        "                image[i] = np.clip(image[i], lower, upper)\n",
        "                image[i] = (image[i] - lower) / (upper - lower) if upper > lower else 0\n",
        "\n",
        "        elif self.scale == 'div255': # Simple division by 255\n",
        "            image = image / 255.0\n",
        "\n",
        "\n",
        "        # Aplica transformações na imagem\n",
        "        if self.transform:\n",
        "            image = image.transpose(1, 2, 0)\n",
        "            image = self.transform(image)\n",
        "        else:\n",
        "            # Assumes image from .npy is (H, W, C), needs to be (C, H, W) for PyTorch\n",
        "            image = image.transpose(2, 0, 1)\n",
        "            image = torch.from_numpy(image)\n",
        "\n",
        "        # Convert mask to tensor and add channel dimension\n",
        "        mask = torch.from_numpy(mask).unsqueeze(0)\n",
        "\n",
        "        return image, mask\n",
        "\n",
        "\n",
        "class AugmentDataset(Dataset):\n",
        "    def __init__(self, dataset, augmentation=\"both\"):\n",
        "        \"\"\"\n",
        "        Applies data augmentation to an existing dataset.\n",
        "        :param dataset: The original dataset (train or validation).\n",
        "        :param augmentation: Augmentation type ('rotation', 'flip', 'both', or None).\n",
        "        \"\"\"\n",
        "        self.dataset = dataset\n",
        "        self.augmentation = augmentation\n",
        "        self.rotations = [0, 90, 180, 270]\n",
        "        self.transforms = []\n",
        "\n",
        "        if augmentation in [\"rotation\", \"both\"]:\n",
        "            self.transforms.extend([T.RandomRotation(degrees=[angle, angle]) for angle in self.rotations])\n",
        "\n",
        "        if augmentation in [\"flip\", \"both\"]:\n",
        "            self.transforms.append(T.RandomHorizontalFlip(p=1.0))\n",
        "            self.transforms.append(T.RandomVerticalFlip(p=1.0))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset) * (len(self.transforms) + 1)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        original_idx = idx % len(self.dataset)\n",
        "        image, mask = self.dataset[original_idx]\n",
        "\n",
        "        # Aplica uma transformação específica com base no índice\n",
        "        transform_idx = idx // len(self.dataset)\n",
        "        if transform_idx > 0:\n",
        "            transform = self.transforms[transform_idx - 1]\n",
        "            image = transform(image)\n",
        "            mask = transform(mask)\n",
        "\n",
        "        return image, mask\n",
        "\n",
        "\n",
        "def get_dataloaders(train_img_dir, train_gt_dir, val_img_dir=None, val_gt_dir=None, batch_size=8, n_workers=4,\n",
        "                    transforms_flag=False, scale='mm', split_data=False, split_ratio=0.8, aug_t=None, aug_v=None):\n",
        "    \"\"\"\n",
        "    Creates dataloaders for training and validation.\n",
        "\n",
        "    :param train_img_dir: Directory for training images.\n",
        "    :param train_gt_dir: Directory for training masks.\n",
        "    :param val_img_dir: Directory for validation images (None if split_data=True).\n",
        "    :param val_gt_dir: Directory for validation masks (None if split_data=True).\n",
        "    :param batch_size: Batch size.\n",
        "    :param n_workers: Number of workers for data loading.\n",
        "    :param transforms_flag: If True, applies specific normalization (mean/std).\n",
        "    :param scale: Normalization method ('mm', 'ss', 'mmpc').\n",
        "    :param split_data: If True, automatically splits the training set into train/validation.\n",
        "    :param split_ratio: Ratio of the training set used for training (e.g., 0.8 = 80% train, 20% val).\n",
        "    :param aug_t: Augmentation type for training ('rotation', 'flip', 'both', or None).\n",
        "    :param aug_v: Augmentation type for validation.\n",
        "    :return: Dataloaders for training and validation.\n",
        "    \"\"\"\n",
        "\n",
        "    means = [9575.1400, 9333.1825, 8279.4832, 17096.3165, 14481.1129, 11567.9320]\n",
        "    stds = [2269.6487, 1782.7761, 1505.7824, 3603.5337, 3851.3550, 3271.9373]\n",
        "\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=means, std=stds),\n",
        "    ]) if transforms_flag else None\n",
        "\n",
        "    full_dataset = UniqueDataset(train_img_dir, train_gt_dir, transform=transform, scale=scale)\n",
        "\n",
        "    if split_data:\n",
        "        train_size = int(split_ratio * len(full_dataset))\n",
        "        val_size = len(full_dataset) - train_size\n",
        "        train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
        "    else:\n",
        "        train_dataset = full_dataset\n",
        "        if val_img_dir and val_gt_dir:\n",
        "            val_dataset = UniqueDataset(val_img_dir, val_gt_dir, transform=transform, scale=scale)\n",
        "        else:\n",
        "            raise ValueError(\"If 'split_data' is False, val_img_dir and val_gt_dir must be provided!\")\n",
        "\n",
        "    # Aplica augmentation apenas ao conjunto de treino\n",
        "    if aug_t:\n",
        "        train_dataset = AugmentDataset(train_dataset, aug_t)\n",
        "    if aug_v:\n",
        "        val_dataset = AugmentDataset(val_dataset, aug_v)\n",
        "    #print(f\"Train size: {len(train_dataset)} | Val size: {len(val_dataset)}\")\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=n_workers)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=n_workers)\n",
        "    #print(f\"Train batches: {len(train_loader)} | Val batches: {len(val_loader)}\")\n",
        "    return train_loader, val_loader"
      ],
      "metadata": {
        "id": "PbvsozSonFrD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import os\n",
        "import torch\n",
        "\n",
        "\n",
        "class JointLoss(nn.Module):\n",
        "    def __init__(self, loss1, loss2, weight1=0.5, weight2=0.5):\n",
        "        \"\"\"\n",
        "        Combines two loss functions with custom weights.\n",
        "        Args:\n",
        "            loss1 (nn.Module): First loss function (e.g., FocalLoss).\n",
        "            loss2 (nn.Module): Second loss function (e.g., DiceLoss).\n",
        "            weight1 (float): Weight for the first loss function.\n",
        "            weight2 (float): Weight for the second loss function.\n",
        "        \"\"\"\n",
        "        super(JointLoss, self).__init__()\n",
        "        self.loss1 = loss1\n",
        "        self.loss2 = loss2\n",
        "        self.weight1 = weight1\n",
        "        self.weight2 = weight2\n",
        "\n",
        "    def forward(self, outputs, targets):\n",
        "        # Calcula as duas perdas\n",
        "        loss1 = self.loss1(outputs, targets)\n",
        "        loss2 = self.loss2(outputs, targets)\n",
        "\n",
        "        # Combina as perdas com os pesos\n",
        "        total_loss = self.weight1 * loss1 + self.weight2 * loss2\n",
        "        return total_loss\n",
        "\n",
        "\n",
        "class ModelCheckpoint:\n",
        "    def __init__(self, checkpoint_dir, max_saves=1, finetune = False):\n",
        "        \"\"\"\n",
        "        Initializes the callback to save only the best model checkpoint.\n",
        "        Args:\n",
        "            checkpoint_dir (str): Directory to save checkpoints.\n",
        "            max_saves (int): Maximum number of checkpoints to keep (set to 1 for best only).\n",
        "            finetune (bool): If True, adds a '_finetune' suffix to the checkpoint name.\n",
        "        \"\"\"\n",
        "        self.checkpoint_dir = checkpoint_dir\n",
        "        self.max_saves = max_saves\n",
        "        self.finetune = finetune\n",
        "        self.best_losses = []  # Lista de tuplas (val_loss, checkpoint_path)\n",
        "        os.makedirs(checkpoint_dir, exist_ok=True)  # Cria o diretório se não existir\n",
        "\n",
        "    def __call__(self, model, avg_val_acc, val_loss, epoch):\n",
        "        \"\"\"\n",
        "        Saves the best checkpoint and removes the previous best.\n",
        "        Args:\n",
        "            model (torch.nn.Module): The model to save.\n",
        "            avg_val_acc (float): The average validation accuracy (IoU).\n",
        "            val_loss (float): The validation loss value.\n",
        "            epoch (int): The current epoch number.\n",
        "        \"\"\"\n",
        "        if not self.finetune:\n",
        "            checkpoint_path = os.path.join(\n",
        "                self.checkpoint_dir, f\"checkpoint_epoch_{epoch}_acc_{avg_val_acc:.4f}_loss_{val_loss:.4f}.pth\"\n",
        "            )\n",
        "        else:\n",
        "            checkpoint_path = os.path.join(\n",
        "                self.checkpoint_dir, f\"checkpoint_epoch_{epoch}_acc_{avg_val_acc:.4f}_loss_{val_loss:.4f}_finetune.pth\"\n",
        "            )\n",
        "\n",
        "        # Se ainda não há checkpoints, salva o primeiro\n",
        "        if not self.best_losses:\n",
        "            self.best_losses.append((val_loss, checkpoint_path))\n",
        "            torch.save(model.state_dict(), checkpoint_path)\n",
        "            return\n",
        "\n",
        "        # Obtém a menor perda já salva\n",
        "        best_loss, best_path = self.best_losses[0]\n",
        "\n",
        "        if val_loss < best_loss:  # Apenas salva se for melhor\n",
        "            # Remove o checkpoint anterior\n",
        "            if os.path.exists(best_path):\n",
        "                os.remove(best_path)\n",
        "\n",
        "            # Atualiza para o novo melhor\n",
        "            self.best_losses = [(val_loss, checkpoint_path)]\n",
        "            torch.save(model.state_dict(), checkpoint_path)\n",
        "            print(f\"Saving new best model: {os.path.basename(checkpoint_path)}\")"
      ],
      "metadata": {
        "id": "S-qXGHgDnM9C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MODEL TRAINING"
      ],
      "metadata": {
        "id": "SZJ4oepMnT2R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "This script orchestrates the training process, bringing together the model, data,\n",
        "and utilities. It features enhanced progress tracking and a robust training loop.\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import os\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "import segmentation_models_pytorch as smp\n",
        "from torchmetrics.classification import BinaryJaccardIndex, BinaryF1Score\n",
        "import torch\n",
        "import gc\n",
        "\n",
        "# Free up memory before starting training\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# --- Global Configurations ---\n",
        "BATCH_SIZE = 8\n",
        "LEARNING_RATE = 0.0001\n",
        "NUM_EPOCHS = 100\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "EARLY_STOP_PATIENCE = 15  # Number of epochs with no improvement to wait before stopping\n",
        "LR_REDUCTION_PATIENCE = 5 # Number of epochs with no improvement to wait before reducing LR\n",
        "\n",
        "# --- Path Configurations ---\n",
        "# IMPORTANT: Replace these paths with your own directories.\n",
        "CHECKPOINT_DIR = \"/path/tile_export/checkpoints\"\n",
        "TRAIN_IMG_DIR = \"/path/tile_export/image/\"\n",
        "TRAIN_GT_DIR = \"/path/tile_export/gt/\"\n",
        "\n",
        "# --- Initial Setup ---\n",
        "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
        "\n",
        "# --- Utility Functions ---\n",
        "def save_last_checkpoint(model, optimizer, epoch, checkpoint_dir):\n",
        "    \"\"\"Saves the state of the model and optimizer at the end of an epoch.\"\"\"\n",
        "    torch.save({\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "    }, os.path.join(checkpoint_dir, \"checkpoint_last.pth\"))\n",
        "\n",
        "# --- Data Loading ---\n",
        "train_loader, val_loader = get_dataloaders(\n",
        "    train_img_dir=TRAIN_IMG_DIR,\n",
        "    train_gt_dir=TRAIN_GT_DIR,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    scale='div255',      # Using simple division by 255 for normalization\n",
        "    split_data=True,     # Automatically split the training data\n",
        "    split_ratio=0.8,     # 80% for training, 20% for validation\n",
        "    aug_t='rotation',        # Apply both rotation and flip augmentation to training data\n",
        "    aug_v='rotation'         # Also apply to validation data for robustness check\n",
        ")\n",
        "\n",
        "# --- Model, Optimizer, and Loss Initialization ---\n",
        "model = UNet(in_channels=3).to(DEVICE)\n",
        "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=5e-3)\n",
        "metric_iou = BinaryJaccardIndex().to(DEVICE)  # Jaccard Index / IoU\n",
        "metric_dice = BinaryF1Score().to(DEVICE)     # F1 Score / Dice Coefficient\n",
        "\n",
        "# Using a single, robust loss function like FocalLoss\n",
        "criterion = smp.losses.FocalLoss(mode='binary')\n",
        "# Example of using a combined loss:\n",
        "# focal_loss = smp.losses.FocalLoss(mode=\"binary\")\n",
        "# iou_loss = smp.losses.JaccardLoss(mode=\"binary\")\n",
        "# criterion = JointLoss(focal_loss, iou_loss, weight1=0.5, weight2=0.5).to(DEVICE)\n",
        "\n",
        "# --- Resume from Checkpoint if available ---\n",
        "start_epoch = 0\n",
        "last_checkpoint_path = os.path.join(CHECKPOINT_DIR, \"checkpoint_last.pth\")\n",
        "if os.path.exists(last_checkpoint_path):\n",
        "    checkpoint = torch.load(last_checkpoint_path)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    start_epoch = checkpoint['epoch']\n",
        "    print(f\"\\n▶ Resuming training from epoch {start_epoch+1}\")\n",
        "\n",
        "# --- Training Setup ---\n",
        "checkpoint_callback = ModelCheckpoint(CHECKPOINT_DIR, max_saves=1)\n",
        "total_start_time = time.time()\n",
        "best_val_loss = float('inf')\n",
        "patience_counter = 0      # Counter for early stopping\n",
        "lr_patience_counter = 0   # Counter for learning rate reduction\n",
        "\n",
        "# --- Informative Header ---\n",
        "print(f\"\\n{'='*65}\")\n",
        "print(f\"Training Started | Device: {DEVICE}\")\n",
        "print(f\"Batch Size: {BATCH_SIZE} | Initial LR: {LEARNING_RATE:.0e}\")\n",
        "print(f\"Epochs: {NUM_EPOCHS} | Checkpoints Dir: {CHECKPOINT_DIR}\")\n",
        "print(f\"Early Stopping Patience: {EARLY_STOP_PATIENCE} epochs | LR Reduction Patience: {LR_REDUCTION_PATIENCE} epochs\")\n",
        "print(f\"{'='*65}\\n\")\n",
        "\n",
        "# --- Main Training Loop ---\n",
        "for epoch in range(start_epoch, NUM_EPOCHS):\n",
        "    epoch_start_time = time.time()\n",
        "    model.train()\n",
        "\n",
        "    # Metric containers for the epoch\n",
        "    batch_times, batch_losses, batch_ious, batch_dices = [], [], [], []\n",
        "\n",
        "    # Training progress bar\n",
        "    train_bar = tqdm(\n",
        "        train_loader,\n",
        "        desc=f\"Epoch {epoch+1:03d}/{NUM_EPOCHS:03d} [Train]\",\n",
        "        bar_format=\"{l_bar}{bar:20}{r_bar}{bar:-20b}\",\n",
        "        unit=\"batch\"\n",
        "    )\n",
        "\n",
        "    for images, masks in train_bar:\n",
        "        batch_start_time = time.time()\n",
        "        images, masks = images.to(DEVICE), masks.to(DEVICE)\n",
        "        if torch.isnan(images).any() or torch.isinf(images).any():\n",
        "          print(\"[Erro] Tensor contém valores inválidos (NaN ou Inf)!\")\n",
        "          break\n",
        "\n",
        "        #print(f\"Shapes => input: {images.shape} | mask: {masks.shape}\")\n",
        "\n",
        "        # Forward pass\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, masks)\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Calculate metrics without tracking gradients\n",
        "        with torch.no_grad():\n",
        "            iou = metric_iou(outputs, masks)\n",
        "            dice = metric_dice(outputs, masks)\n",
        "\n",
        "        # Update statistics\n",
        "        batch_times.append(time.time() - batch_start_time)\n",
        "        batch_losses.append(loss.item())\n",
        "        batch_ious.append(iou.item())\n",
        "        batch_dices.append(dice.item())\n",
        "\n",
        "        # Update progress bar postfix\n",
        "        train_bar.set_postfix({\n",
        "            'loss': f\"{loss.item():.4f}\",\n",
        "            'iou': f\"{iou.item():.4f}\",\n",
        "            'dice': f\"{dice.item():.4f}\",\n",
        "            'b_time': f\"{batch_times[-1]:.2f}s\",\n",
        "            'lr': f\"{optimizer.param_groups[0]['lr']:.1e}\"\n",
        "        })\n",
        "\n",
        "    # --- End of Training Epoch ---\n",
        "    avg_train_loss = sum(batch_losses) / len(train_loader)\n",
        "    avg_train_iou = sum(batch_ious) / len(train_loader)\n",
        "    avg_train_dice = sum(batch_dices) / len(train_loader)\n",
        "    epoch_time = time.time() - epoch_start_time\n",
        "\n",
        "    # --- Validation Phase ---\n",
        "    model.eval()\n",
        "    val_loss, val_iou, val_dice = 0.0, 0.0, 0.0\n",
        "    val_bar = tqdm(\n",
        "        val_loader,\n",
        "        desc=f\"Epoch {epoch+1:03d}/{NUM_EPOCHS:03d} [Validate]\",\n",
        "        bar_format=\"{l_bar}{bar:20}{r_bar}{bar:-20b}\",\n",
        "        unit=\"batch\"\n",
        "    )\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, masks in val_bar:\n",
        "            images, masks = images.to(DEVICE), masks.to(DEVICE)\n",
        "            outputs = model(images)\n",
        "\n",
        "            loss = criterion(outputs, masks)\n",
        "            iou = metric_iou(outputs, masks)\n",
        "            dice = metric_dice(outputs, masks)\n",
        "\n",
        "            val_loss += loss.item()\n",
        "            val_iou += iou.item()\n",
        "            val_dice += dice.item()\n",
        "\n",
        "            val_bar.set_postfix({\n",
        "                'loss': f\"{loss.item():.4f}\",\n",
        "                'iou': f\"{iou.item():.4f}\",\n",
        "                'dice': f\"{dice.item():.4f}\"\n",
        "            })\n",
        "\n",
        "    avg_val_loss = val_loss / len(val_loader)\n",
        "    avg_val_iou = val_iou / len(val_loader)\n",
        "    avg_val_dice = val_dice / len(val_loader)\n",
        "\n",
        "    # --- Early Stopping and Learning Rate Scheduling Logic ---\n",
        "    if avg_val_loss < best_val_loss:\n",
        "        best_val_loss = avg_val_loss\n",
        "        patience_counter = 0\n",
        "        lr_patience_counter = 0  # Reset both counters on improvement\n",
        "        checkpoint_callback(model, avg_val_iou, avg_val_loss, epoch + 1)\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        lr_patience_counter += 1\n",
        "\n",
        "        # Early stopping check\n",
        "        if patience_counter >= EARLY_STOP_PATIENCE:\n",
        "            print(f\"\\nEarly Stopping triggered at epoch {epoch+1}! No improvement for {EARLY_STOP_PATIENCE} consecutive epochs.\")\n",
        "            break\n",
        "\n",
        "        # Learning rate reduction check\n",
        "        if lr_patience_counter >= LR_REDUCTION_PATIENCE:\n",
        "            current_lr = optimizer.param_groups[0]['lr']\n",
        "            new_lr = current_lr * 0.5 # Halve the learning rate\n",
        "            for param_group in optimizer.param_groups:\n",
        "                param_group['lr'] = new_lr\n",
        "            print(f\"\\nReducing learning rate to {new_lr:.1e} after {LR_REDUCTION_PATIENCE} epochs without improvement.\")\n",
        "            lr_patience_counter = 0  # Reset the LR counter after reduction\n",
        "\n",
        "    # --- Time and Memory Statistics ---\n",
        "    total_time = time.time() - total_start_time\n",
        "    avg_batch_time = sum(batch_times) / len(batch_times)\n",
        "    epochs_left = NUM_EPOCHS - (epoch + 1)\n",
        "    # Estimated Time of Arrival (ETA)\n",
        "    eta = epochs_left * (total_time / (epoch + 1 - start_epoch)) if epoch >= start_epoch else 0\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        gpu_mem = torch.cuda.max_memory_allocated() / (1024 ** 3)  # in GB\n",
        "        torch.cuda.reset_peak_memory_stats()\n",
        "    else:\n",
        "        gpu_mem = 0\n",
        "\n",
        "    # --- Print Epoch Summary ---\n",
        "    print(f\"\\nEPOCH {epoch+1:03d}/{NUM_EPOCHS:03d} SUMMARY [{epoch_time:.1f}s]\")\n",
        "    print(f\"  Train => Loss: {avg_train_loss:.4f} | IoU: {avg_train_iou:.4f} | Dice: {avg_train_dice:.4f}\")\n",
        "    print(f\"  Valid => Loss: {avg_val_loss:.4f} | IoU: {avg_val_iou:.4f} | Dice: {avg_val_dice:.4f}\")\n",
        "    print(f\"  Patience: {patience_counter}/{EARLY_STOP_PATIENCE} | LR Patience: {lr_patience_counter}/{LR_REDUCTION_PATIENCE}\")\n",
        "    if gpu_mem > 0:\n",
        "        print(f\"  Avg Batch Time: {avg_batch_time:.2f}s | Peak GPU Memory: {gpu_mem:.2f}GB\")\n",
        "    print(f\"  Elapsed Time: {time.strftime('%H:%M:%S', time.gmtime(total_time))} | ETA: {time.strftime('%H:%M:%S', time.gmtime(eta))}\\n\")\n",
        "\n",
        "    # Save the last checkpoint for resuming\n",
        "    save_last_checkpoint(model, optimizer, epoch + 1, CHECKPOINT_DIR)\n",
        "\n",
        "# --- End of Training ---\n",
        "print(f\"\\n{'='*65}\")\n",
        "print(f\"Training Complete!\")\n",
        "print(f\"Total Time: {time.strftime('%H:%M:%S', time.gmtime(time.time()-total_start_time))}\")\n",
        "if checkpoint_callback.best_losses:\n",
        "    best_model_path = sorted(checkpoint_callback.best_losses)[0][1]\n",
        "    print(f\"Best Model Saved: {os.path.basename(best_model_path)}\")\n",
        "else:\n",
        "    print(\"No best model was saved (training may have been interrupted early).\")\n",
        "print(f\"{'='*65}\")"
      ],
      "metadata": {
        "id": "JmfrEyqWnW8I"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}