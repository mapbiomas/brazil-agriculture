{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WfPm4Wqvh2c2"
      },
      "outputs": [],
      "source": [
        "!pip install geedim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5-CDmuDbt38H"
      },
      "outputs": [],
      "source": [
        "import ee\n",
        "from geedim.mask import MaskedImage\n",
        "import geedim as gd\n",
        "import geemap\n",
        "import os\n",
        "\n",
        "ee.Authenticate()\n",
        "ee.Initialize(project='your-project')\n",
        "\n",
        "\n",
        "gd.Initialize()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "drLxqKVW5yZ5"
      },
      "outputs": [],
      "source": [
        "# Complete grid of the area containing the references\n",
        "\n",
        "gridCitrus = [ee.Feature(\n",
        "            ee.Geometry.Polygon(\n",
        "                [[[-50.20835949121198, -21.570663125073093],\n",
        "                  [-50.72050304571127, -22.200959744423816],\n",
        "                  [-50.68565127421229, -22.932608048112623],\n",
        "                  [-49.866185262251165, -22.98077261280791],\n",
        "                  [-49.503636434126165, -23.283862968048616],\n",
        "                  [-49.553074910688665, -23.837735761251672],\n",
        "                  [-49.349827840376165, -24.00845815352208],\n",
        "                  [-49.311375691938665, -24.244083909496336],\n",
        "                  [-49.228978231001165, -24.53425536155134],\n",
        "                  [-48.97547494422113, -24.725445298459096],\n",
        "                  [-48.489741931783264, -24.731592346469217],\n",
        "                  [-48.14549742311739, -24.322593378428273],\n",
        "                  [-47.724949667812986, -24.327091965922907],\n",
        "                  [-47.437269496812824, -23.950077157042],\n",
        "                  [-47.43613448523368, -23.415067073529507],\n",
        "                  [-46.38255063280743, -22.88038075582629],\n",
        "                  [-46.84095509689042, -21.77193402420044],\n",
        "                  [-46.74382413604112, -21.64922289378126],\n",
        "                  [-46.583149087212995, -21.676025201368024],\n",
        "                  [-46.52959073760362, -21.53429954530793],\n",
        "                  [-46.52684415557237, -21.456355270321527],\n",
        "                  [-46.62022794463487, -21.444851762543262],\n",
        "                  [-46.665546548150495, -21.37453284907612],\n",
        "                  [-46.709491860650495, -21.403943056391473],\n",
        "                  [-46.81798185088487, -21.380926875344564],\n",
        "                  [-46.923725259087995, -21.429512340911486],\n",
        "                  [-47.01298917510362, -21.4231204394093],\n",
        "                  [-47.279047810917184, -20.851889595965073],\n",
        "                  [-47.158021161900756, -20.61731520045495],\n",
        "                  [-47.316961073087256, -20.464215228056993],\n",
        "                  [-47.2752758647711, -20.1865217085684],\n",
        "                  [-47.4419103909856, -20.052672951898675],\n",
        "                  [-47.68526037786939, -20.042406662064312],\n",
        "                  [-47.843497060032334, -20.003421766796027],\n",
        "                  [-47.87321154982845, -20.11540753664955],\n",
        "                  [-48.00348974169029, -20.153345290766932],\n",
        "                  [-48.1016275790738, -20.167119881732656],\n",
        "                  [-48.219046505700014, -20.13188414230849],\n",
        "                  [-48.53605498010481, -20.144689623967082],\n",
        "                  [-48.82107541404858, -20.176639307140984],\n",
        "                  [-48.85112364968661, -20.326517796093476],\n",
        "                  [-48.863429958090656, -20.42976338645249],\n",
        "                  [-48.91421486953524, -20.455778893571708],\n",
        "                  [-48.98844085547703, -20.391500988910167],\n",
        "                  [-48.98156584016366, -20.217639457557173],\n",
        "                  [-49.071436090298484, -20.187353525937127],\n",
        "                  [-49.138351551753715, -20.321631351818983],\n",
        "                  [-49.25479104141024, -20.332206656915023],\n",
        "                  [-49.35938823026913, -20.015255179209973],\n",
        "                  [-50.42504163610109, -19.878605814486914],\n",
        "                  [-51.96312757360109, -21.400165662068165]]]),\n",
        "            {\n",
        "              \"system:index\": \"0\"\n",
        "            }),\n",
        "        ee.Feature(\n",
        "            ee.Geometry.Polygon(\n",
        "                [[[-51.55216470857867, -22.675353633729504],\n",
        "                  [-51.58512369295367, -22.502914376355886],\n",
        "                  [-52.39811197420367, -22.11159640458579],\n",
        "                  [-53.11771646639117, -22.584089156562552],\n",
        "                  [-53.08475748201617, -22.639869094502327],\n",
        "                  [-52.72770181795367, -22.634799125926747],\n",
        "                  [-52.17838541170367, -22.65507787746228],\n",
        "                  [-52.12894693514117, -22.513063833701953]]]),\n",
        "            {\n",
        "              \"system:index\": \"1\"\n",
        "            })]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CffrI-4ahmkS"
      },
      "outputs": [],
      "source": [
        "# Setting\n",
        "year = 2024\n",
        "\n",
        "startDate = '2020-01-01'\n",
        "endDate = '2020-12-31'\n",
        "startDate_aim = f'{year}-01-01'\n",
        "endDate_aim = f'{year}-12-31'\n",
        "\n",
        "chirp_scale, chirp_size = 30, 1024\n",
        "chirp_size_m = chirp_scale * chirp_size\n",
        "\n",
        "cloudCoverValue = 80\n",
        "uf_code = 'SP'\n",
        "\n",
        "output_folder = f'DATASET_CITRUS_PERC_{uf_code}_{year}'\n",
        "\n",
        "# Collection and input layers\n",
        "ref_map = ee.FeatureCollection('projects/assets/reference_map')\n",
        "estados = ee.FeatureCollection('regions/ibge_estados_2019')\n",
        "proj = gridCitrus.first().geometry().projection()\n",
        "\n",
        "# Function\n",
        "def filter_landsat(path, roi, start, end, cloud_max):\n",
        "    return ee.ImageCollection(path) \\\n",
        "        .filterDate(start, end) \\\n",
        "        .filterBounds(roi) \\\n",
        "        .filter(ee.Filter.lt('CLOUD_COVER_LAND', cloud_max))\n",
        "\n",
        "\n",
        "def padronize_band_names(image):\n",
        "    spacecraft_id = image.get('SPACECRAFT_ID')\n",
        "\n",
        "    old_band_names = ee.Dictionary({\n",
        "        'LANDSAT_5': ['B1', 'B2', 'B3', 'B4', 'B5', 'B7', 'B6', 'QA_PIXEL'],\n",
        "        'LANDSAT_7': ['B1', 'B2', 'B3', 'B4', 'B5', 'B7', 'B6_VCID_1', 'QA_PIXEL'],\n",
        "        'LANDSAT_8': ['B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B10', 'QA_PIXEL'],\n",
        "        'LANDSAT_9': ['B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B10', 'QA_PIXEL']\n",
        "    }).get(spacecraft_id)\n",
        "\n",
        "    new_band_names = ['blue', 'green', 'red', 'nir', 'swir1', 'swir2', 'tir1', 'BQA']\n",
        "\n",
        "    return ee.Algorithms.If(\n",
        "        old_band_names,\n",
        "        image.select(ee.List(old_band_names), new_band_names),\n",
        "        image\n",
        "    )\n",
        "\n",
        "\n",
        "def mask_clouds(image):\n",
        "    qa = image.select('BQA')\n",
        "    mask = (qa.bitwiseAnd(1 << 3)\n",
        "            .And(qa.bitwiseAnd(1 << 8).Or(qa.bitwiseAnd(1 << 9)))\n",
        "            .Or(qa.bitwiseAnd(1 << 1))\n",
        "            .Or(qa.bitwiseAnd(1 << 4).And(qa.bitwiseAnd(1 << 10).Or(qa.bitwiseAnd(1 << 11))))\n",
        "            .Or(qa.bitwiseAnd(1 << 5))\n",
        "            .Or(qa.bitwiseAnd(1 << 7))\n",
        "            .Or(qa.bitwiseAnd(1 << 14).And(qa.bitwiseAnd(1 << 15))))\n",
        "    return image.updateMask(mask.Not())\n",
        "\n",
        "def normalize_band(band_name, image, p1, p99):\n",
        "    return image.select(band_name).unitScale(p1, p99).clamp(0, 1).rename(f'{band_name}_norm')\n",
        "\n",
        "def get_evi2(image):\n",
        "    evi2 = image.expression(\n",
        "        '2.5 * (NIR - RED) / (NIR + 2.4 * RED + 1)',\n",
        "        {\n",
        "            'NIR': image.select('nir_norm'),\n",
        "            'RED': image.select('red_norm')\n",
        "        }).rename('evi2')\n",
        "    return image.addBands(evi2)\n",
        "\n",
        "def get_ndwi(image):\n",
        "    ndwi = image.expression(\n",
        "        '(NIR - SWIR1) / (NIR + SWIR1)',\n",
        "        {\n",
        "            'NIR': image.select('nir_norm'),\n",
        "            'SWIR1': image.select('swir1_norm')\n",
        "        }).rename('ndwi')\n",
        "    return image.addBands(ndwi)\n",
        "\n",
        "# Interest region\n",
        "roi = estados.filter(ee.Filter.eq('SIGLA_UF', uf_code))\n",
        "\n",
        "\n",
        "# Polygon division function (split_pol)\n",
        "def split_pol(ft):\n",
        "    id_property_name_in_grid_rice = 'id'\n",
        "    ft_original_id_val = ft.get(id_property_name_in_grid_rice)\n",
        "\n",
        "    id_value_computed = ee.Algorithms.If(\n",
        "        ft_original_id_val,\n",
        "        ft_original_id_val,\n",
        "        ee.String('grid_').cat(ee.String(ft.get('system:index')))\n",
        "    )\n",
        "    ft_original_id_eeString = ee.String(id_value_computed)\n",
        "\n",
        "    geom_reproject = ft.transform(proj.atScale(chirp_size), 1)\n",
        "\n",
        "    def map_over_cells(ftg):\n",
        "        ftg = ee.Feature(ftg)\n",
        "        cell_idx = ee.String(ftg.get('system:index')).split(',').join('_').replace('-', '1')\n",
        "        unique_id_for_export = ft_original_id_eeString.cat('_').cat(cell_idx)\n",
        "        return ftg.copyProperties(ft).set('id', unique_id_for_export)\n",
        "\n",
        "    return geom_reproject.geometry().coveringGrid(proj, chirp_size_m).map(map_over_cells)\n",
        "\n",
        "bigs_splitted = gridCitrus.map(split_pol).flatten()\n",
        "\n",
        "\n",
        "print(gridCitrus.getInfo())\n",
        "\n",
        "# Reference\n",
        "reference = ee.Image(0).paint(ref_map, 1).rename('reference').clip(roi)\n",
        "\n",
        "\n",
        "# Collection Landsat\n",
        "l5 = filter_landsat(\"LANDSAT/LT05/C02/T1_TOA\", roi, \"2000-01-01\", \"2011-10-01\", cloudCoverValue)\n",
        "l7a = filter_landsat(\"LANDSAT/LE07/C02/T1_TOA\", roi, \"2000-01-01\", \"2003-05-31\", cloudCoverValue)\n",
        "l7b = filter_landsat(\"LANDSAT/LE07/C02/T1_TOA\", roi, \"2011-10-01\", \"2013-03-01\", cloudCoverValue)\n",
        "l8 = filter_landsat(\"LANDSAT/LC08/C02/T1_TOA\", roi, \"2013-03-01\", \"2030-01-01\", cloudCoverValue)\n",
        "l9 = filter_landsat(\"LANDSAT/LC09/C02/T1_TOA\", roi, \"2019-03-01\", \"2030-01-01\", cloudCoverValue)\n",
        "\n",
        "\n",
        "collection = l8.merge(l9).merge(l7a).merge(l7b).merge(l5) \\\n",
        "    .map(lambda img: ee.Image(padronize_band_names(img))) \\\n",
        "    .map(mask_clouds) \\\n",
        "    .filterDate(startDate, endDate)\n",
        "\n",
        "median_ref = collection.median()\n",
        "\n",
        "\n",
        "masked = median_ref.updateMask(reference)\n",
        "bands = ['blue', 'green', 'red', 'nir', 'swir1', 'swir2']\n",
        "percentiles = masked.select(bands).reduceRegion(\n",
        "    reducer=ee.Reducer.percentile([1, 99]),\n",
        "    geometry=roi.geometry(),\n",
        "    scale=chirp_scale,\n",
        "    maxPixels=1e13\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "# Assemble and process the referece collection\n",
        "aim_collection = l8.merge(l9).merge(l7a).merge(l7b).merge(l5) \\\n",
        "    .map(lambda img: ee.Image(padronize_band_names(img))) \\\n",
        "    .map(mask_clouds) \\\n",
        "    .filterDate(startDate_aim, endDate_aim)\n",
        "\n",
        "median = aim_collection.median()\n",
        "\n",
        "\n",
        "\n",
        "# Normalized bands\n",
        "\n",
        "norm_bands = []\n",
        "for b in bands:\n",
        "    p1 = ee.Number(percentiles.get(f'{b}_p1'))\n",
        "    p99 = ee.Number(percentiles.get(f'{b}_p99'))\n",
        "    norm_bands.append(normalize_band(b, median, p1, p99))\n",
        "\n",
        "normalized = ee.Image(norm_bands).toFloat()\n",
        "\n",
        "mosaic_unet = normalized.select(['red_norm', 'nir_norm', 'swir1_norm'])\n",
        "\n",
        "\n",
        "image_to_export = mosaic_unet.unmask().multiply(255).uint8()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xo6BsLm7sdUP"
      },
      "outputs": [],
      "source": [
        "# Exportation in iiles with geedim\n",
        "\n",
        "grid_list = bigs_splitted.aggregate_array('id').getInfo()\n",
        "\n",
        "output_folder = '/path/tile_export'\n",
        "\n",
        "if not os.path.exists(output_folder):\n",
        "    os.makedirs(output_folder)\n",
        "\n",
        "gd_image = gd.MaskedImage(image_to_export)\n",
        "\n",
        "for i, grid_id in enumerate(grid_list):\n",
        "  gd_image.download(\n",
        "      filename=f'{output_folder}/mosaic_{grid_id}_mosaic.tif',\n",
        "      region=bigs_splitted.filter(ee.Filter.eq('id', grid_id)).geometry(),\n",
        "      scale=chirp_scale,\n",
        "      crs='EPSG:3857',\n",
        "      overwrite=True,\n",
        "      bands=['red_norm', 'nir_norm', 'swir1_norm'],\n",
        "      resampling='near',\n",
        "      dtype='uint8',\n",
        "      scale_offset=None\n",
        "  )\n",
        "\n",
        "print(f\"\\nExport started. The imagesAs imagens will be save at path '{output_folder}'.\")\n",
        "print(f\"A file will be generated .tif for each of the {bigs_splitted.size().getInfo()} geometries of your grid.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HSpVmFs4wSGp"
      },
      "source": [
        "# INFERENCE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQoPNNeZwWDf"
      },
      "source": [
        "## UNET"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A8VlaV5fwVYp"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    \"\"\"\n",
        "    A standard U-Net architecture for semantic segmentation.\n",
        "    Args:\n",
        "        in_channels (int): Number of input channels (e.g., 3 for RGB).\n",
        "        out_channels (int): Number of output classes (e.g., 1 for binary segmentation).\n",
        "        init_features (int): Number of features in the first convolutional layer.\n",
        "        no_drop (bool): If True, dropout layers are disabled (replaced with Identity).\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels=3, out_channels=1, init_features=64, no_drop=True):\n",
        "        super(UNet, self).__init__()\n",
        "        self.no_drop = no_drop\n",
        "\n",
        "        features = init_features\n",
        "\n",
        "        self.encoder1 = UNet._block(in_channels, features, name=\"enc1\")\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.drop1 = nn.Dropout(0.25) if not no_drop else nn.Identity()\n",
        "\n",
        "        self.encoder2 = UNet._block(features, features * 2, name=\"enc2\")\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.drop2 = nn.Dropout(0.25) if not no_drop else nn.Identity()\n",
        "\n",
        "        self.encoder3 = UNet._block(features * 2, features * 4, name=\"enc3\")\n",
        "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.drop3 = nn.Dropout(0.5) if not no_drop else nn.Identity()\n",
        "\n",
        "        self.encoder4 = UNet._block(features * 4, features * 8, name=\"enc4\")\n",
        "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.drop4 = nn.Dropout(0.5) if not no_drop else nn.Identity()\n",
        "\n",
        "        self.bottleneck = UNet._block(features * 8, features * 16, name=\"bottleneck\")\n",
        "        self.drop5 = nn.Dropout(0.5) if not no_drop else nn.Identity()\n",
        "\n",
        "        self.upconv4 = nn.ConvTranspose2d(features * 16, features * 8, kernel_size=2, stride=2)\n",
        "        self.decoder4 = UNet._block(features * 16, features * 8, name=\"dec4\")\n",
        "\n",
        "        self.upconv3 = nn.ConvTranspose2d(features * 8, features * 4, kernel_size=2, stride=2)\n",
        "        self.decoder3 = UNet._block(features * 8, features * 4, name=\"dec3\")\n",
        "\n",
        "        self.upconv2 = nn.ConvTranspose2d(features * 4, features * 2, kernel_size=2, stride=2)\n",
        "        self.decoder2 = UNet._block(features * 4, features * 2, name=\"dec2\")\n",
        "\n",
        "        self.upconv1 = nn.ConvTranspose2d(features * 2, features, kernel_size=2, stride=2)\n",
        "        self.decoder1 = UNet._block(features * 2, features, name=\"dec1\")\n",
        "\n",
        "        # Camada final\n",
        "        self.conv = nn.Conv2d(in_channels=features, out_channels=out_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Encoder\n",
        "        enc1 = self.drop1(self.encoder1(x))\n",
        "        enc2 = self.drop2(self.encoder2(self.pool1(enc1)))\n",
        "        enc3 = self.drop3(self.encoder3(self.pool2(enc2)))\n",
        "        enc4 = self.drop4(self.encoder4(self.pool3(enc3)))\n",
        "\n",
        "        # Bottleneck\n",
        "        bottleneck = self.drop5(self.bottleneck(self.pool4(enc4)))\n",
        "\n",
        "        # Decoder\n",
        "        dec4 = self.upconv4(bottleneck)\n",
        "        dec4 = torch.cat((dec4, enc4), dim=1)\n",
        "        dec4 = self.decoder4(dec4)\n",
        "\n",
        "        dec3 = self.upconv3(dec4)\n",
        "        dec3 = torch.cat((dec3, enc3), dim=1)\n",
        "        dec3 = self.decoder3(dec3)\n",
        "\n",
        "        dec2 = self.upconv2(dec3)\n",
        "        dec2 = torch.cat((dec2, enc2), dim=1)\n",
        "        dec2 = self.decoder2(dec2)\n",
        "\n",
        "        dec1 = self.upconv1(dec2)\n",
        "        dec1 = torch.cat((dec1, enc1), dim=1)\n",
        "        dec1 = self.decoder1(dec1)\n",
        "\n",
        "        return self.conv(dec1)\n",
        "\n",
        "    @staticmethod\n",
        "    def _block(in_channels, features, name):\n",
        "        \"\"\"\n",
        "        Creates a standard U-Net block: Conv -> BN -> ReLU -> Conv -> BN -> ReLU\n",
        "        \"\"\"\n",
        "        return nn.Sequential(\n",
        "            nn.Conv2d(in_channels, features, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(features),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(features, features, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(features),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "96FnmvLnwZIM"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "This section defines the `GeoTIFFPredictor` class, which uses the trained model\n",
        "to perform segmentation on large, new GeoTIFF images. It employs a sliding\n",
        "window approach with blending to produce seamless prediction maps.\n",
        "\"\"\"\n",
        "\n",
        "import rasterio\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from rasterio.windows import Window\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "class GeoTIFFPredictor:\n",
        "    \"\"\"\n",
        "    Performs windowed inference on GeoTIFF files using a trained PyTorch model.\n",
        "    Handles patch-based prediction with overlapping windows and smooth blending.\n",
        "    \"\"\"\n",
        "    def __init__(self, model, device, window_size=256, overlap=64):\n",
        "        \"\"\"\n",
        "        Initializes the predictor.\n",
        "        Args:\n",
        "            model (torch.nn.Module): Trained segmentation model.\n",
        "            device (str or torch.device): Device to run the model on ('cpu' or 'cuda').\n",
        "            window_size (int): The size of the processing window (patch).\n",
        "            overlap (int): Overlap between windows for smooth blending.\n",
        "        \"\"\"\n",
        "        self.model = model.to(device)\n",
        "        self.device = device\n",
        "        self.window_size = window_size\n",
        "        self.overlap = overlap\n",
        "        self.stride = window_size - overlap\n",
        "        # A ramp for smooth blending at the edges\n",
        "        self.ramp = np.linspace(0, 1, overlap // 2)\n",
        "        self.band_stats = {}\n",
        "\n",
        "    def normalize_band(self, band, scale='mm'):\n",
        "        \"\"\"\n",
        "        Normalizes a single image band according to the selected method.\n",
        "        Args:\n",
        "            band (np.ndarray): Band data.\n",
        "            scale (str): Normalization method: 'mm' (min-max), 'ss' (standard score), or 'div255'.\n",
        "        Returns:\n",
        "            np.ndarray: Normalized band.\n",
        "        \"\"\"\n",
        "        if scale == 'mm':\n",
        "            min_val, max_val = band.min(), band.max()\n",
        "            return (band - min_val) / (max_val - min_val) if max_val > min_val else np.zeros_like(band, dtype=np.float32)\n",
        "        elif scale == 'ss':\n",
        "            mean_val, std_val = band.mean(), band.std()\n",
        "            return (band - mean_val) / std_val if std_val > 0 else np.zeros_like(band, dtype=np.float32)\n",
        "        elif scale == 'div255':\n",
        "            return band.astype(np.float32) / 255.0\n",
        "        else:\n",
        "            raise ValueError(\"Invalid normalization option. Use 'mm', 'ss', or 'div255'.\")\n",
        "\n",
        "    def get_blend_weights(self, h, w, y_start, x_start, src_h, src_w):\n",
        "        \"\"\"\n",
        "        Generates spatial blending weights for a patch to ensure smooth transitions.\n",
        "        Weights are 1 in the center and ramp down to 0 at the edges of the overlap area.\n",
        "        \"\"\"\n",
        "        weights = np.ones((h, w), dtype=np.float32)\n",
        "        half_overlap = self.overlap // 2\n",
        "\n",
        "        # Apply ramps to the edges of the patch\n",
        "        if y_start > 0:\n",
        "            weights[:half_overlap, :] *= self.ramp[:, np.newaxis]\n",
        "        if y_start + h < src_h:\n",
        "            weights[-half_overlap:, :] *= self.ramp[::-1, np.newaxis]\n",
        "        if x_start > 0:\n",
        "            weights[:, :half_overlap] *= self.ramp[np.newaxis, :]\n",
        "        if x_start + w < src_w:\n",
        "            weights[:, -half_overlap:] *= self.ramp[np.newaxis, ::-1]\n",
        "\n",
        "        return torch.from_numpy(weights).to(self.device)\n",
        "\n",
        "    def predict_geotiff(self, input_path, output_path, return_probs=True, scaler='ss'):\n",
        "        \"\"\"\n",
        "        Runs prediction over a GeoTIFF file using sliding window inference.\n",
        "        Args:\n",
        "            input_path (str): Path to the input GeoTIFF file.\n",
        "            output_path (str): Path to save the predicted output GeoTIFF.\n",
        "            return_probs (bool): If True, saves a probability map; otherwise, saves a binary mask.\n",
        "            scaler (str): Normalization method ('mm', 'ss', or 'div255').\n",
        "        \"\"\"\n",
        "        with rasterio.open(input_path) as src:\n",
        "            full_image = src.read().astype(np.float32)\n",
        "            print(full_image.shape)\n",
        "            if full_image.shape[0] == 4:\n",
        "              full_image = full_image[:-1, :, :]\n",
        "            print(full_image.shape)\n",
        "            # Normalize each band of the image\n",
        "            normalized_image = np.array([self.normalize_band(full_image[b], scaler) for b in range(full_image.shape[0])])\n",
        "\n",
        "            orig_height, orig_width = normalized_image.shape[1], normalized_image.shape[2]\n",
        "\n",
        "            # Pad the image to ensure it's divisible by the window size\n",
        "            pad_h = (self.stride - (orig_height - self.overlap) % self.stride) % self.stride\n",
        "            pad_w = (self.stride - (orig_width - self.overlap) % self.stride) % self.stride\n",
        "            padded_image = np.pad(normalized_image, ((0, 0), (0, pad_h), (0, pad_w)), mode='reflect')\n",
        "            padded_height, padded_width = padded_image.shape[1], padded_image.shape[2]\n",
        "\n",
        "            # Create empty arrays to accumulate predictions and weights\n",
        "            full_pred = np.zeros((padded_height, padded_width), dtype=np.float32)\n",
        "            full_count = np.zeros((padded_height, padded_width), dtype=np.float32)\n",
        "\n",
        "            # Generate all window offsets\n",
        "            offsets = [\n",
        "                (y, x) for y in range(0, padded_height - self.overlap, self.stride)\n",
        "                       for x in range(0, padded_width - self.overlap, self.stride)\n",
        "            ]\n",
        "\n",
        "            # Perform model inference on each window\n",
        "            for y_start, x_start in tqdm(offsets, desc=\"Processing windows\"):\n",
        "                y_end, x_end = y_start + self.window_size, x_start + self.window_size\n",
        "                chip = padded_image[:, y_start:y_end, x_start:x_end]\n",
        "                input_tensor = torch.from_numpy(chip).unsqueeze(0).to(self.device)\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    output = self.model(input_tensor)\n",
        "                    pred = torch.sigmoid(output).squeeze().cpu().numpy()\n",
        "\n",
        "                h, w = pred.shape\n",
        "                weights = self.get_blend_weights(h, w, y_start, x_start, padded_height, padded_width)\n",
        "\n",
        "                full_pred[y_start:y_end, x_start:x_end] += pred * weights.cpu().numpy()\n",
        "                full_count[y_start:y_end, x_start:x_end] += weights.cpu().numpy()\n",
        "\n",
        "            # Normalize the prediction by the sum of weights to get the final blended result\n",
        "            full_pred = np.divide(full_pred, full_count, where=full_count > 0)\n",
        "\n",
        "            # Remove padding to return to original dimensions\n",
        "            final_pred = full_pred[:orig_height, :orig_width]\n",
        "\n",
        "            if not return_probs:\n",
        "                final_pred = (final_pred > 0.5).astype(np.uint8)\n",
        "\n",
        "            self.save_geotiff(output_path, final_pred, src.profile, return_probs)\n",
        "\n",
        "    def save_geotiff(self, output_path, data, profile, return_probs):\n",
        "        \"\"\"\n",
        "        Saves the prediction output as a GeoTIFF file using original metadata.\n",
        "        \"\"\"\n",
        "        profile.update({\n",
        "            'driver': 'GTiff',\n",
        "            'height': data.shape[0],\n",
        "            'width': data.shape[1],\n",
        "            'count': 1,\n",
        "            'dtype': 'float32' if return_probs else 'uint8',\n",
        "            'nodata': None,\n",
        "            'compress': 'lzw'\n",
        "        })\n",
        "        with rasterio.open(output_path, 'w', **profile) as dst:\n",
        "            dst.write(data, 1)\n",
        "\n",
        "def process_directory(input_dir, output_dir, predictor, return_probs=True, scaler='ss'):\n",
        "    \"\"\"\n",
        "    Processes all .tif files in a directory using the GeoTIFFPredictor.\n",
        "    \"\"\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    input_files = list(Path(input_dir).glob(\"*.tif\"))\n",
        "\n",
        "    for input_file in tqdm(input_files, desc=\"Processing files\"):\n",
        "        output_file = Path(output_dir) / f\"{input_file.stem}_pred.tif\"\n",
        "        print(f\"\\nPredicting on {input_file.name} -> {output_file.name}\")\n",
        "        predictor.predict_geotiff(str(input_file), str(output_file), return_probs, scaler)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z8f6ZSluwpcA"
      },
      "outputs": [],
      "source": [
        "UF_MODEL = \"RS\"\n",
        "UF_PREDICT = \"RS\"\n",
        "\n",
        "MODEL_CHECKPOINT_NAME = \"trained_model_name\" #\n",
        "\n",
        "CHECKPOINT_DIR = f\"/path/trained_model/checkpoints/\" # SC\n",
        "INPUT_TIFF_DIR = f\"/path/tiff_input\"\n",
        "OUTPUT_PRED_DIR = f\"/path/tiff_input/pred\"\n",
        "\n",
        "# Construct the full path to the model checkpoint\n",
        "checkpoint_path = os.path.join(CHECKPOINT_DIR, MODEL_CHECKPOINT_NAME)\n",
        "\n",
        "# Ensure output directory exists\n",
        "os.makedirs(OUTPUT_PRED_DIR, exist_ok=True)\n",
        "\n",
        "# --- Model and Predictor Setup ---\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Initialize the model and load the trained weights\n",
        "model = UNet(in_channels=3).to(device)\n",
        "model.load_state_dict(torch.load(checkpoint_path, map_location=device))\n",
        "model.eval() # Set the model to evaluation mode\n",
        "\n",
        "# Initialize the predictor\n",
        "predictor = GeoTIFFPredictor(model, device, window_size=256, overlap=32)\n",
        "\n",
        "# Process all GeoTIFF files in the specified directory\n",
        "print(f\"Starting inference on files in: {INPUT_TIFF_DIR}\")\n",
        "process_directory(\n",
        "    input_dir=INPUT_TIFF_DIR,\n",
        "    output_dir=OUTPUT_PRED_DIR,\n",
        "    predictor=predictor,\n",
        "    return_probs=True,      # Save as a probability map (float32)\n",
        "    scaler='div255'         # Use simple division by 255 for normalization\n",
        ")\n",
        "print(\"Inference complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_pcpYnF6yxQt"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import rasterio\n",
        "import os\n",
        "\n",
        "def visualize_predictions(input_dir, prediction_dir):\n",
        "    \"\"\"\n",
        "    Visualiza as imagens de entrada e suas predições lado a lado.\n",
        "\n",
        "    Args:\n",
        "        input_dir (str): Diretório contendo as imagens de entrada (.tif).\n",
        "        prediction_dir (str): Diretório contendo as imagens de predição (.tif).\n",
        "    \"\"\"\n",
        "    input_files = sorted([f for f in os.listdir(input_dir) if f.endswith('.tif')])\n",
        "    prediction_files = sorted([f for f in os.listdir(prediction_dir) if f.endswith('.tif')])\n",
        "\n",
        "    # Assuming a one-to-one correspondence between input and prediction files\n",
        "    # based on naming convention (prediction file has \"_pred\" suffix)\n",
        "    # You might need to adjust this logic based on your exact file naming\n",
        "    prediction_map = {f.replace('_pred.tif', '.tif'): f for f in prediction_files}\n",
        "\n",
        "    if not input_files:\n",
        "        print(f\"Nenhum arquivo .tif encontrado em {input_dir}\")\n",
        "        return\n",
        "    if not prediction_files:\n",
        "        print(f\"Nenhum arquivo .tif encontrado em {prediction_dir}\")\n",
        "        return\n",
        "\n",
        "    print(f\"Encontrados {len(input_files)} arquivos de entrada e {len(prediction_files)} arquivos de predição.\")\n",
        "\n",
        "    for input_filename in input_files:\n",
        "        if input_filename in prediction_map:\n",
        "            prediction_filename = prediction_map[input_filename]\n",
        "\n",
        "            input_path = os.path.join(input_dir, input_filename)\n",
        "            prediction_path = os.path.join(prediction_dir, prediction_filename)\n",
        "\n",
        "            try:\n",
        "                with rasterio.open(input_path) as src_input, \\\n",
        "                     rasterio.open(prediction_path) as src_pred:\n",
        "\n",
        "                    input_img = src_input.read()\n",
        "                    pred_img = src_pred.read(1) # Read the single band of prediction\n",
        "\n",
        "                    # Assuming input is multi-channel, potentially > 3\n",
        "                    # Select bands to display (e.g., first 3) or create a composite\n",
        "                    # This example displays the first 3 bands if available\n",
        "                    if input_img.shape[0] >= 3:\n",
        "                         display_input = input_img[:3].transpose(1, 2, 0) # Rearrange bands for matplotlib\n",
        "                         # Simple normalization for display\n",
        "                         display_input = (display_input - display_input.min()) / (display_input.max() - display_input.min())\n",
        "                    elif input_img.shape[0] == 1:\n",
        "                         display_input = input_img[0]\n",
        "                    else:\n",
        "                         print(f\"Warning: Could not display input image {input_filename}. Needs 1 or at least 3 bands.\")\n",
        "                         continue\n",
        "\n",
        "\n",
        "                    fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
        "\n",
        "                    # Display Input Image\n",
        "                    if input_img.shape[0] >= 3:\n",
        "                        axes[0].imshow(display_input)\n",
        "                    elif input_img.shape[0] == 1:\n",
        "                         axes[0].imshow(display_input, cmap='gray') # Use grayscale for single band\n",
        "                    axes[0].set_title(f'Imagem de Entrada: {input_filename}')\n",
        "                    axes[0].axis('off')\n",
        "\n",
        "                    # Display Prediction Image\n",
        "                    # For probability maps, use a colormap like 'viridis' or 'hot'\n",
        "                    # For binary masks (if saved as uint8), use 'gray'\n",
        "                    cmap = 'viridis' if src_pred.profile['dtype'] == 'float32' else 'gray'\n",
        "                    axes[1].imshow(pred_img, cmap=cmap)\n",
        "                    axes[1].set_title(f'Predição: {prediction_filename}')\n",
        "                    axes[1].axis('off')\n",
        "\n",
        "                    plt.tight_layout()\n",
        "                    plt.show()\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Erro ao processar {input_filename} ou {prediction_filename}: {e}\")\n",
        "        else:\n",
        "            print(f\"Predição não encontrada para a imagem de entrada: {input_filename}\")\n",
        "\n",
        "# Define the directories containing the original images and the predictions\n",
        "# Make sure these match the directories used in your inference code\n",
        "input_image_directory = INPUT_TIFF_DIR # Assuming images are downloaded here\n",
        "predicted_image_directory = OUTPUT_PRED_DIR\n",
        "\n",
        "# Run the visualization\n",
        "visualize_predictions(input_image_directory, predicted_image_directory)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
